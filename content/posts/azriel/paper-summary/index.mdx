---
title: "Interpreting BLEU/NIST Scores: How Much Improvement Do We Need to Have a Better System?"
slug: "/azriel/paper-summary"
date: 2025-11-16
author: azriel
description: "My paper summary about a critical practical issue in machine translation (MT) evaluation"
tags:
  - paper summary
---


## Citation

_Replace this text with a complete citation_

<table>
  <caption>
    Citation summary
  </caption>
  <thead>
  <tr>
    <th></th>
    <th></th>
  </tr>
  </thead>
<tbody>
  <tr>
    <td><b>Paper</b></td>
    <td>Interpreting BLEU/NIST Scores: How Much Improvement Do We Need to Have a Better System?</td>
  </tr>
  <tr>
    <td><b>Authors</b></td>
    <td>Y. Zhang and Stephan Vogel and Alexander H. Waibel</td>
  </tr>
  <tr>
    <td><b>Year published</b></td>
    <td>2004</td>
  </tr>
  <tr>
    <td><b>Venue</b></td>
    <td>International Conference on Language Resources and Evaluation</td>
  </tr>
  <tr>
    <td><b>Paper URL</b></td>
    <td>https://api.semanticscholar.org/CorpusID:8080832</td>
  </tr>
  <tr>
    <td><b>Code URL</b></td>
    <td></td>
  </tr>
</tbody>
</table>

## Description

_In the article, "Interpreting BLEU/NIST Scores: How Much Improvement do We Need to Have a Better System?‚Äù by Zhang, Vogel, & Waibel (2004), the authors examine automatic evaluation metrics for NLP translation. The findings of the research suggest improvements in these scoring metrics may not result in meaningful improvement of language translation quality or fluency. The authors provide considerable evidence to suggest BLEU and NIST scoring alone may not reflect fluency. The research provides methods to interpret these metrics and suggests a nuanced approach, from a human point of view may be needed to correctly determine the performance of language translation performance. This research introduces the problem, that human-centric feedback even if not be automated, may still be needed to develop excellent language translation tools._

## Motivation

_As a student interested in NLP, I've learned from this research article that BLEU and NIST scores can be helpful for assessing NLP translation models. I am fascinated by languages and understanding the capabilities and limitations of models may help me in a future career or project building translation tools. Other courses have introduced metrics such as METEOR, BERTScore and TER, but I have not studied BLEU and NIST scoring. The article helps me compare metrics to understand the strenghs and weaknesses of NLP metrics. The paper provides statistical data and was interesting as it implies measuring NLP accuracy may require a variety of scoring methods. _
< !-- NOTE: don't use an LLM to generate this! üôÉ -->

## LLM

<table>
  <caption>
    LLM model summary
  </caption>
  <thead>
  <tr>
    <th></th>
    <th></th>
  </tr>
  </thead>
<tbody>
  <tr>
    <th><b>LLM model</b></th>
    <td>huggingchat</td>
  </tr>
  <tr>
    <th><b>LLM model version</b></th>
    <td>Qwen3</td>
  </tr>
  <tr>
    <th><b>Model/service URL</b></th>
    <td>https://huggingface.co/chat/</td>
  </tr>
  <tr>
    <th><b>Why this model?</b></th>
    <td>It is free and I pick among many models.</td>
  </tr>
</tbody>
</table>


#### Description (LLM)

_In the LLM's words, what is this paper about?_

##### Prompt

```markdown
prompt here
```

#### What are the authors proposing?

##### Prompt

```markdown
The authors are proposing a way to determine how much improvement in BLEU or NIST score is needed for a machine translation system to be truly better, according to human judgment.
```

#### What is the motivation for the work?

##### Prompt

```markdown
The motivation is that researchers were using BLEU and NIST scores to compare machine translation systems, but didn‚Äôt know how much improvement counts as real progress.
```

#### What is the approach or innovation?

##### Prompt

```markdown
The approach is to compare automatic scores (BLEU/NIST) with human judgments of translation quality, and use statistics to find out when score differences are meaningful.
```

#### What are the results and how do they compare with competing approaches?

##### Prompt

```markdown
BLEU and NIST scores are highly correlated with human judgments ‚Äî meaning they do a good job ranking translation systems correctly.
A difference of about 0.5 BLEU points or more (on large test sets) is likely to be meaningful.
On smaller test sets, you need larger differences (e.g., 1‚Äì2 points) to be sure one system is better.
With enough data, even small improvements can be statistically significant.
```

#### Is the comparison fair?

##### Prompt

```markdown
Yes, for its goal. The paper isn‚Äôt trying to prove BLEU is better than other metrics.
```

#### What are the takeaways according to the authors?

##### Prompt

```markdown
1) Not all score improvements are meaningful.
A tiny gain in BLEU (like +0.1 or +0.2) probably does not mean a better system ‚Äî it could just be noise.

2) A difference of about 0.5 BLEU points or more can be meaningful ‚Äî but only if you‚Äôre using a large test set (e.g., 1,000+ sentences).

3) Test set size matters.
The more sentences you evaluate on, the more confident you can be in small improvements.

4) Use statistical significance testing.
Always check if the score difference is statistically significant ‚Äî don‚Äôt just compare numbers by eye.

5) BLEU and NIST are reliable when used correctly.
They correlate well with human judgment, so they‚Äôre useful ‚Äî as long as you interpret them carefully.
```

#### What are the takeaways according to you?

##### Prompt

```markdown
Don‚Äôt claim your system is better just because BLEU went up a little.
Only trust improvements that are large enough and statistically significant.
```

#### Would you use this?  If so, how/where would you use this?

##### Prompt

```markdown
Yes, I would use this ‚Äî especially the paper‚Äôs guidelines ‚Äî whenever evaluating or comparing machine translation systems.
```

##### Prompt

```markdown
I would follow its rules for fair, honest evaluation ‚Äî in research, development, and review.
```

#### What problems remain and what are the next steps?

##### Prompt

```markdown
Problems that remain:

1) BLEU is still limited.
It counts matching words, but doesn‚Äôt fully capture meaning, grammar, or fluency. A translation can score high and still be wrong or awkward.

2) Human judgment is the gold standard ‚Äî but it‚Äôs slow and expensive.
We still need faster, cheaper ways to predict what humans would say.

3) Small improvements are hard to detect.
Even with large test sets, tiny but real gains (e.g., +0.2 BLEU) may not be statistically significant.

4) BLEU works poorly for some languages.
It‚Äôs better for languages close to English (like French) than for ones very different (like Chinese or Arabic).

5) BLEU doesn‚Äôt handle paraphrasing well.
If a translation says the same thing in different words, BLEU may score it low.

Next steps (then and now):

1) Use significance testing routinely.
Always check if score differences are real ‚Äî this became standard practice after this paper.

2) Develop better metrics.
This work helped motivate newer metrics like METEOR, TER, BLEURT, COMET, and BERTScore, which use semantics and neural models to better match human judgment.

3) Combine automatic scores with human evaluation.
Use BLEU for quick checks, but validate key results with human ratings.

4) Test on larger, diverse datasets.
To get reliable results, evaluate on more data and different domains (news, speech, medical, etc.).


```

### Experience using the LLM

_Describe your process for using the LLM.  How did the LLM perform?_
 
#### Errors and limitations of the LLM 

_Where did it fall short or make mistakes?_
