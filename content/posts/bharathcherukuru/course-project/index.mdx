---
title: "Building a RAG System pipelines using open source LLMs"
slug: "/bharathcherukuru/course-project"
date: 2025-11-16
author: Bharath Cherukuru
description: "Our submission for the course project is building a RAG pipeline by using multiple embedding systems where the RAG enhances language model responses by retrieving relevant information from the dataset provided along with the query given."
tags:
  - course project
---

<table>
  <caption>
    Course Project Info
  </caption>
<tbody>
  <tr>
    <th>Code Repository URL</th>
    <td>https://github.com/uazhlt-ms-program/ling-582-fall-2025-course-project-code-bya</td>
  </tr>
  <tr>
    <th>Demo URL (optional)</th>
    <td></td>
  </tr>
  <tr>
    <th>BYA</th>
    <td></td>
  </tr>
</tbody>
</table>


## Project description

_See [the rubric](https://parsertongue.org/courses/snlp-2/assignments/rubrics/course-project/final/#reproducibility).  Use subsections to organize components of the description._
We are creating a RAG pipeline as the course project, where we will use publicly available data. Firstly we will be working with multiple embedding systems, where all the data used is embedded by a embedding system and will set up a retriever to get the most matched information with the query . 

Later we will connect with one of the open source LLMs available and build a RAG pipeline. Along with the query, the retreived chunks are feeded to the LLMs. So to know the performance of the pipeline, we will manually evaluate by making a test set of sample questions, their expected answers and the answers we get from LLMs and compare them. Here we will be repeating with different embedding models and compare the results manually. 

As an extended analysis, we are also planning to work on hallucinations,  by checking if the LLMs generates facts not found in the retrieved sources, or did the retriever fail or the LLM is making up things. And also compare RAG vs Non RAG, like how the results are when we are using RAG and when we are not using RAG.


## TimeLine:

Week 1 ( Nov 3 - Nov 9) : Forming a team and discussing about the interests of different team members to work on as a project.

week 2 ( Nov 10- Nov 16): Finalizing the project and choosing different embedding systems and open source LLMs.

Week 3 ( Nov 17–Nov 23): Preprocess and chunk documents, then set up vector embeddings with an initial model (such as E5). Working on different embedding models like BGE, GTE. 

Week 4 (Nov 24–Nov 30): Setting up a vector database, manually evaluating the quality metrics. Integrating the cost effective LLM and build RAG pipelines for question answering.

Week 5 (Dec 1–Dec 7) : Analyze results, track hallucination/errors, and finalize the repo for the submission and check up on rubrics to make the final version.

## Contributors:

Cherukuru Bharath
Yashvi Kommidi
Abhiram Varma Nandimandalam

## Summary of individual contributions
<table>
  <thead>
  <tr>
    <th>Team member</th>
    <th>Role/contributions</th>
  </tr>
  </thead>
<tbody>
  <tr>
    <th><b>???</b></th>
    <td>???</td>
  </tr>
  <tr>
    <th><b>???</b></th>
    <td>???</td>
  </tr>
  <tr>
    <th><b>???</b></th>
    <td>???</td>
  </tr>
</tbody>
</table>


## Results

_See [the rubric](https://parsertongue.org/courses/snlp-2/assignments/rubrics/course-project/final/#results)_

## Error analysis

_See [the rubric](https://parsertongue.org/courses/snlp-2/assignments/rubrics/course-project/final/#error-analysis)_

## Reproducibility

_See [the rubric](https://parsertongue.org/courses/snlp-2/assignments/rubrics/course-project/final/#reproducibility)_
_If you'ved covered this in your code repository's README, you can simply link to that document with a note._

## Future improvements

_See [the rubric](https://parsertongue.org/courses/snlp-2/assignments/rubrics/course-project/final/#proposal-for-future-improvements)_