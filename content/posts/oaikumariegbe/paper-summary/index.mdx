---
title: "An Insightful Summary of **Large Language Models are Zero-Shot Reasoners**"
slug: "/oaikumariegbe/paper-summary"
date: 2025-11-09
author: Oghenevovwe Ikumariegbe
description: "This blog post summarizes a paper showing that minimal cueing‚Äîsuch as the prompt ‚ÄúLet‚Äôs think step by step‚Äù‚Äîcan induce reasoning in large language models"
tags:
  - paper summary
---


## Citation

Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large language models are zero-shot reasoners. Advances in neural information processing systems, 35, 22199-22213.

<table>
  <caption>
    Citation summary
  </caption>
  <thead>
  <tr>
    <th></th>
    <th></th>
  </tr>
  </thead>
<tbody>
  <tr>
    <td><b>Paper</b></td>
    <td>Large Language Models are Zero-Shot Reasoners</td>
  </tr>
  <tr>
    <td><b>Authors</b></td>
    <td>Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa</td>
  </tr>
  <tr>
    <td><b>Year published</b></td>
    <td>2022</td>
  </tr>
  <tr>
    <td><b>Venue</b></td>
    <td>Advances in Neural Information Processing Systems (NeurIPS)</td>
  </tr>
  <tr>
    <td><b>Paper URL</b></td>
    <td>https://proceedings.neurips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html</td>
  </tr>
  <tr>
    <td><b>Code URL</b></td>
    <td>https://github.com/kojima-takeshi188/zero_shot_cot</td>
  </tr>
</tbody>
</table>

## Description
The paper investigates how Large Language Models (LLMs) 
can be prompted to perform reasoning without 
any few-shot examples.
By introducing a simple cue&mdash;‚ÄúLet‚Äôs think step by step‚Äù&mdash;the authors show that 
LLMs can generate coherent intermediate reasoning in a purely zero-shot setting. 
Through a series of experiments and comparisons against few-shot prompting, 
the paper demonstrates that this cueing is often sufficient 
to turn LLMs into effective _zero-shot reasoners_.


## Motivation

The motivation for selecting this paper is that it connects 
directly to the course project and uncovers a previously 
underexplored capability of LLMs&emdash;zero-shot reasoning.
It also demonstrates a practical way to access the 
parametric knowledge LLMs acquire during training 
using simple prompts.
< !-- NOTE: don't use an LLM to generate this! üôÉ -->

## LLM

<table>
  <caption>
    LLM model summary
  </caption>
  <thead>
  <tr>
    <th></th>
    <th></th>
  </tr>
  </thead>
<tbody>
  <tr>
    <th><b>LLM model</b></th>
    <td>???</td>
  </tr>
  <tr>
    <th><b>LLM model version</b></th>
    <td>???</td>
  </tr>
  <tr>
    <th><b>Model/service URL</b></th>
    <td>???</td>
  </tr>
  <tr>
    <th><b>Why this model?</b></th>
    <td>???</td>
  </tr>
</tbody>
</table>


#### Description (LLM)

_In the LLM's words, what is this paper about?_

##### Prompt

```markdown
prompt here
```

#### What are the authors proposing?

##### Prompt

```markdown
prompt here
```

#### What is the motivation for the work?

##### Prompt

```markdown
prompt here
```

#### What is the approach or innovation?

##### Prompt

```markdown
prompt here
```

#### What are the results and how do they compare with competing approaches?

##### Prompt

```markdown
prompt here
```

#### Is the comparison fair?

##### Prompt

```markdown
prompt here
```

#### What are the takeaways according to the authors?

##### Prompt

```markdown
prompt here
```

#### What are the takeaways according to you?
TODO
- Ask if Shreya has confirmed with the Professor if this is for the LLM or for us.
##### Prompt

```markdown
prompt here
```

#### Would you use this?  If so, how/where would you use this?

##### Prompt

```markdown
prompt here
```

##### Prompt

```markdown
prompt here
```

#### What problems remain and what are the next steps?

##### Prompt

```markdown
prompt here
```

### Experience using the LLM

_Describe your process for using the LLM.  How did the LLM perform?_
 
#### Errors and limitations of the LLM 

_Where did it fall short or make mistakes?_
