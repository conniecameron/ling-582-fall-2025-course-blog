---
title: "Comparing Large Language Models and Human Annotators in latent content analysis of sentiment, political learning, emotional intensity and sarcasm"
slug: "/ankithan/paper-summary"
date: 2025-11-09
author: Ankitha Namala
description: "My incredible paper summary on Comparing Large Language Models and Human Annotators in latent content analysis"
tags:
  - paper summary
---


## Citation

TY  - JOUR
AU  - Bojić, Ljubiša
AU  - Zagovora, Olga
AU  - Zelenkauskaite, Asta
AU  - Vuković, Vuk
AU  - Čabarkapa, Milan
AU  - Veseljević Jerković, Selma
AU  - Jovančević, Ana
PY  - 2025
DA  - 2025/04/03
TI  - Comparing large Language models and human annotators in latent content analysis of sentiment, political leaning, emotional intensity and sarcasm
JO  - Scientific Reports
SP  - 11477
VL  - 15
IS  - 1
AB  - In the era of rapid digital communication, vast amounts of textual data are generated daily, demanding efficient methods for latent content analysis to extract meaningful insights. Large Language Models (LLMs) offer potential for automating this process, yet comprehensive assessments comparing their performance to human annotators across multiple dimensions are lacking. This study evaluates the inter-rater reliability, consistency, and quality of seven state-of-the-art LLMs. These include variants of OpenAI’s GPT-4, Gemini, Llama-3.1-70B, and Mixtral 8 × 7B. Their performance is compared to human annotators in analyzing sentiment, political leaning, emotional intensity, and sarcasm detection. The study involved 33 human annotators and eight LLM variants assessing 100 curated textual items. This resulted in 3,300 human and 19,200 LLM annotations. LLM performance was also evaluated across three-time points to measure temporal consistency. The results reveal that both humans and most LLMs exhibit high inter-rater reliability in sentiment analysis and political leaning assessments, with LLMs demonstrating higher reliability than humans. In emotional intensity, LLMs displayed higher reliability compared to humans, though humans rated emotional intensity significantly higher. Both groups struggled with sarcasm detection, evidenced by low reliability. Most LLMs showed excellent temporal consistency across all dimensions, indicating stable performance over time. This research concludes that LLMs, especially GPT-4, can effectively replicate human analysis in sentiment and political leaning, although human expertise remains essential for emotional intensity interpretation. The findings demonstrate the potential of LLMs for consistent and high-quality performance in certain areas of latent content analysis.
SN  - 2045-2322
UR  - https://doi.org/10.1038/s41598-025-96508-3
DO  - 10.1038/s41598-025-96508-3
ID  - Bojić2025

<table>
  <caption>
    Citation summary
  </caption>
  <thead>
  <tr>
    <th></th>
    <th></th>
  </tr>
  </thead>
<tbody>
  <tr>
    <td><b>Paper</b></td>
    <td>Comparing large Language models and human annotators in latent content analysis of sentiment, political leaning, emotional intensity and sarcasm</td>
  </tr>
  <tr>
    <td><b>Authors</b></td>
    <td>Ljubiša Bojić, Olga Zagovora, Milan Čabarkapa, Asta Zelenkauskaite, Selma Veseljević Jerković, Vuk Vuković, Ana Jovančević</td>
  </tr>
  <tr>
    <td><b>Year published</b></td>
    <td>2025</td>
  </tr>
  <tr>
    <td><b>Venue</b></td>
    <td>Scientific Reports</td>
  </tr>
  <tr>
    <td><b>Paper URL</b></td>
    <td>https://doi.org/10.1038/s41598-025-96508-3</td>
  </tr>
  <tr>
    <td><b>Code URL</b></td>
    <td>https://github.com/zagovora/LLM-Laten-Content-Analysis.git</td>
  </tr>
</tbody>
</table>

## Description

This paper compares performance of LLMs to human annotators in analysing social-media content along different dimensions like sentiment, political leaning, emotional intensity and sarcasm. The authors collected online posts, had both humans and various LLMs label them and measure the level of agreement. The level of agreement between them is measured using statistical methods like the inter rater reliability calculated using the Krippendorff's alpha value and correlation values. The suthors tru to assess the potential of LLMs evaluation over human analysis across these dimensions. This is helpful to evaluate if LLMs are a viable substitute or supplements to humans in content analysis tasks.

## Motivation

I selected this paper becuase it closely aligns with the ongoing research I am part of. We are trying to analyse how LLMs and human annotators evaluate creativity in multiplayer games. We also use the inter-rater reliabilty and Fleiss Kappa value to evaluate them. This paper's focus on comparing LLMs and humans in interpreting complex, subjective Language directly connects to that work. Studying this comparison will help me to understand the strengths and limitations of LLMs where human judgement is required and whether they can be as reliable as the human annotators. As the study includes different dimensions than what I am currently studying, it helps me to understand the capabilities of LLMs across various dimensions.

## LLM

<table>
  <caption>
    LLM model summary
  </caption>
  <thead>
  <tr>
    <th></th>
    <th></th>
  </tr>
  </thead>
<tbody>
  <tr>
    <th><b>LLM model</b></th>
    <td>???</td>
  </tr>
  <tr>
    <th><b>LLM model version</b></th>
    <td>???</td>
  </tr>
  <tr>
    <th><b>Model/service URL</b></th>
    <td>???</td>
  </tr>
  <tr>
    <th><b>Why this model?</b></th>
    <td>???</td>
  </tr>
</tbody>
</table>


#### Description (LLM)

_In the LLM's words, what is this paper about?_

##### Prompt

```markdown
prompt here
```

#### What are the authors proposing?

##### Prompt

```markdown
prompt here
```

#### What is the motivation for the work?

##### Prompt

```markdown
prompt here
```

#### What is the approach or innovation?

##### Prompt

```markdown
prompt here
```

#### What are the results and how do they compare with competing approaches?

##### Prompt

```markdown
prompt here
```

#### Is the comparison fair?

##### Prompt

```markdown
prompt here
```

#### What are the takeaways according to the authors?

##### Prompt

```markdown
prompt here
```

#### What are the takeaways according to you?

##### Prompt

```markdown
prompt here
```

#### Would you use this?  If so, how/where would you use this?

##### Prompt

```markdown
prompt here
```

##### Prompt

```markdown
prompt here
```

#### What problems remain and what are the next steps?

##### Prompt

```markdown
prompt here
```

### Experience using the LLM

_Describe your process for using the LLM.  How did the LLM perform?_
 
#### Errors and limitations of the LLM 

_Where did it fall short or make mistakes?_
