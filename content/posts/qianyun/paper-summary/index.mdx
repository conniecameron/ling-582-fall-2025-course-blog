---
title: "EXAMPLE: My Paper Summary"
slug: "/Qianyun/paper-summary"
date: 2023-11-16
author: DQY
description: This paper introduces mBLIP, a lightweight multilingual extension of BLIP-2. Instead of retraining a full vision-language model, the authors reuse the original BLIP-2 image encoder and align it to a multilingual language model using small amounts of translated captioning and VQA data.
  - paper summary
---


## Citation

Gregor Geigle, Abhay Jain, Radu Timofte, and Goran Glavaš. 2024. mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs. In Proceedings of the 3rd Workshop on Advances in Language and Vision Research (ALVR), pages 7–25, Bangkok, Thailand. Association for Computational Linguistics.

<table>
  <caption>
    Citation summary
  </caption>
  <thead>
  <tr>
    <th></th>
    <th></th>
  </tr>
  </thead>
<tbody>
  <tr>
    <td><b>Paper</b></td>
    <td>mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs</td>
  </tr>
  <tr>
    <td><b>Authors</b></td>
    <td>Gregor Geigle, Abhay Jain, Radu Timofte, Goran Glavaš</td>
  </tr>
  <tr>
    <td><b>Year published</b></td>
    <td>2024</td>
  </tr>
  <tr>
    <td><b>Venue</b></td>
    <td>ALVR | WS</td>
  </tr>
  <tr>
    <td><b>Paper URL</b></td>
    <td>https://aclanthology.org/2024.alvr-1.2/</td>
  </tr>
  <tr>
    <td><b>Code URL</b></td>
    <td>N/A</td>
  </tr>
</tbody>
</table>

## Description

_In your own words, what is this paper about?_
This paper presents mBLIP, an efficient multilingual adaptation of BLIP-2. Rather than training a new vision-language model from scratch, the authors retain BLIP-2’s pretrained image encoder and realign it to a multilingual LLM using LoRA/QLoRA and a small set of machine-translated image–text data.
The method drastically reduces training cost while enabling image captioning, VQA, and image–text matching across 95 languages.

## Motivation

_Why did you select this paper?_
I chose this paper because its overall approach is directly related to the goals of my own project.
I am fine-tuning a BLIP-style model for chart-to-text caption generation, and this paper explores a very similar idea: adapting BLIP-2 to a new domain in an efficient and lightweight way.
This paper provides both conceptual inspiration and practical techniques that can help shape my chart-captioning system! :)

## LLM

<table>
  <caption>
    LLM model summary
  </caption>
  <thead>
  <tr>
    <th></th>
    <th></th>
  </tr>
  </thead>
<tbody>
  <tr>
    <th><b>LLM model</b></th>
    <td>JAN UI</td>
  </tr>
  <tr>
    <th><b>LLM model version</b></th>
    <td>Jan-v1-4B-Q4_K_M</td>
  </tr>
  <tr>
    <th><b>Model/service URL</b></th>
    <td>???</td>
  </tr>
  <tr>
    <th><b>Why this model?</b></th>
    <td>???</td>
  </tr>
</tbody>
</table>


#### Description (LLM)

_In the LLM's words, what is this paper about?_

##### Prompt

```markdown
prompt here
```

#### What are the authors proposing?

##### Prompt

```markdown
prompt here
```

#### What is the motivation for the work?

##### Prompt

```markdown
prompt here
```

#### What is the approach or innovation?

##### Prompt

```markdown
prompt here
```

#### What are the results and how do they compare with competing approaches?

##### Prompt

```markdown
prompt here
```

#### Is the comparison fair?

##### Prompt

```markdown
prompt here
```

#### What are the takeaways according to the authors?

##### Prompt

```markdown
prompt here
```

#### What are the takeaways according to you?

##### Prompt

```markdown
prompt here
```

#### Would you use this?  If so, how/where would you use this?

##### Prompt

```markdown
prompt here
```

##### Prompt

```markdown
prompt here
```

#### What problems remain and what are the next steps?

##### Prompt

```markdown
prompt here
```

### Experience using the LLM

_Describe your process for using the LLM.  How did the LLM perform?_
 
#### Errors and limitations of the LLM 

_Where did it fall short or make mistakes?_
