---
title: "TruthfulRAG — Draft Summary"
slug: "/abhiramn/paper-summary"
date: 2025-11-15
author: Abhiram Varma Nandimandalam
description: "My incredible paper summary of TruthfulRAG"
tags:
  - paper summary
---

## Citation

Liu, S., Shang, Y., & Zhang, X. (2025). **TruthfulRAG: Resolving Factual-level Conflicts in Retrieval-Augmented Generation with Knowledge Graphs.**  
arXiv:2511.10375 [cs.CL].
@misc{liu2025truthfulragresolvingfactuallevelconflicts,
title={TruthfulRAG: Resolving Factual-level Conflicts in Retrieval-Augmented Generation with Knowledge Graphs},
author={Shuyi Liu and Yuming Shang and Xi Zhang},
year={2025},
eprint={2511.10375},
archivePrefix={arXiv},
primaryClass={cs.CL},
url={https://arxiv.org/abs/2511.10375},
}  
Paper URL: https://arxiv.org/abs/2511.10375  
DOI: https://doi.org/10.48550/arXiv.2511.10375

<table>
  <caption>Citation summary</caption>
  <thead>
    <tr>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
        <b>Paper</b>
      </td>
      <td>TruthfulRAG</td>
    </tr>
    <tr>
      <td>
        <b>Authors</b>
      </td>
      <td>Shuyi Liu, Yuming Shang, Xi Zhang</td>
    </tr>
    <tr>
      <td>
        <b>Year published</b>
      </td>
      <td>2025</td>
    </tr>
    <tr>
      <td>
        <b>Venue</b>
      </td>
      <td>arXiv (accepted at AAAI 2026)</td>
    </tr>
    <tr>
      <td>
        <b>Paper URL</b>
      </td>
      <td>https://arxiv.org/abs/2511.10375</td>
    </tr>
    <tr>
      <td>
        <b>Code URL</b>
      </td>
      <td>Not provided</td>
    </tr>
  </tbody>
</table>

## Description

This paper introduces **TruthfulRAG**, a retrieval-augmented generation system that uses knowledge graphs and entropy-based conflict detection to resolve factual contradictions between an LLM’s internal knowledge and newly retrieved external documents. Instead of working at the token or embedding level, the method builds triple-based reasoning paths, filters conflicting evidence, and improves factual correctness on multi-hop and time-sensitive tasks.

## Motivation

I am interested in this paper because it tackles a core weakness in RAG: models blending outdated internal knowledge with newly retrieved facts, which often harms reliability. The idea of turning both parametric knowledge and retrieved content into knowledge graphs feels powerful, because it lets the system reason over explicit facts instead of just raw text. I am personally drawn to methods that make RAG more trustworthy and robust, especially in changing real‑world environments where information can conflict. Graph-based reasoning also fits my interest in more interpretable, structured approaches to LLMs rather than purely black‑box prompting. Overall, this paper sits exactly at the intersection of my interests in RAG, LLM reliability, and knowledge-graph–driven reasoning.

## LLM

<table>
  <caption>LLM model summary</caption>
  <thead>
    <tr>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>
        <b>LLM model</b>
      </th>
      <td>???</td>
    </tr>
    <tr>
      <th>
        <b>LLM model version</b>
      </th>
      <td>???</td>
    </tr>
    <tr>
      <th>
        <b>Model/service URL</b>
      </th>
      <td>???</td>
    </tr>
    <tr>
      <th>
        <b>Why this model?</b>
      </th>
      <td>???</td>
    </tr>
  </tbody>
</table>

#### Description (LLM)

_In the LLM's words, what is this paper about?_

##### Prompt

```markdown
prompt here
```

#### What are the authors proposing?

##### Prompt

```markdown
prompt here
```

#### What is the motivation for the work?

##### Prompt

```markdown
prompt here
```

#### What is the approach or innovation?

##### Prompt

```markdown
prompt here
```

#### What are the results and how do they compare with competing approaches?

##### Prompt

```markdown
prompt here
```

#### Is the comparison fair?

##### Prompt

```markdown
prompt here
```

#### What are the takeaways according to the authors?

##### Prompt

```markdown
prompt here
```

#### What are the takeaways according to you?

##### Prompt

```markdown
prompt here
```

#### Would you use this? If so, how/where would you use this?

##### Prompt

```markdown
prompt here
```

##### Prompt

```markdown
prompt here
```

#### What problems remain and what are the next steps?

##### Prompt

```markdown
prompt here
```

### Experience using the LLM

_Describe your process for using the LLM. How did the LLM perform?_

#### Errors and limitations of the LLM

_Where did it fall short or make mistakes?_
