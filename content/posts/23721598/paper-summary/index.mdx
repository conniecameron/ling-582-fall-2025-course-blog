---
title: "Paper Summary: A Semantic Parsing Framework for End-to-End Time Normalization"
slug: "/23721598/paper-summary"
date: 2025-11-15
author: Yanyan Dong
description: "The paper summary for a NeurIPS 2025 paper on semantic parsing and time normalization."
tags:
  - paper summary
---

## Citation

Su, X., Yu, S., Howard, P., & Bethard, S. (2025). A Semantic Parsing Framework for End-to-End Time Normalization. In Proceedings of the Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025). arXiv:2507.06450. [https://arxiv.org/abs/2507.06450](https://arxiv.org/abs/2507.06450)

<table>
  <caption>
    Citation summary
  </caption>
  <thead>
  <tr>
    <th></th>
    <th></th>
  </tr>
  </thead>
<tbody>
  <tr>
    <td><b>Paper</b></td>
    <td>A Semantic Parsing Framework for End-to-End Time Normalization</td>
  </tr>
  <tr>
    <td><b>Authors</b></td>
    <td>Xin Su, Sungduk Yu, Phillip Howard, Steven Bethard</td>
  </tr>
  <tr>
    <td><b>Year published</b></td>
    <td>2025</td>
  </tr>
  <tr>
    <td><b>Venue</b></td>
    <td>NeurIPS 2025</td>
  </tr>
  <tr>
    <td><b>Paper URL</b></td>
    <td>https://arxiv.org/abs/2507.06450</td>
  </tr>
  <tr>
    <td><b>Code URL</b></td>
    <td>N/A</td>
  </tr>
</tbody>
</table>

## Description

This paper presents a new approach that allows models to interpret natural-language time expressions more accurately. Traditional methods often struggle with complex or ambiguous temporal phrases, and although large language models are powerful, they can still be inconsistent when handling time expressions.

The key idea of this paper is to convert each time expression into an executable ‚Äútime python program‚Äù using a framework called Semantically Compositional Annotation of Temporal Expressions (SCATE). This paper uses an LLM to generate SCATE code for many sentences and automatically filters out errors by executing the code, which produces a high-quality set of synthetic training data. It then combines this synthetic data with existing human-annotated data to train a much smaller model. Experiments show that the resulting small model ultimately outperforms the large language model on the time-normalization task.

## Motivation

I selected this paper because:
- Time expressions are important in many NLP tasks (e.g., event extraction, QA, clinical NLP), yet accurate time normalization is still a hard problem.
- This paper introduces a structured and interpretable approach using the SCATE framework.
- This paper proposes a novel use of LLMs to generate semantic training data and train a smaller, more reliable model.
- As a recent NeurIPS 2025 accepted paper, the work is high-quality and solid.

< !-- NOTE: don't use an LLM to generate this! üôÉ -->

---

(To be completed in final submission.)

## LLM

<table>
  <caption>
    LLM model summary
  </caption>
  <thead>
  <tr>
    <th></th>
    <th></th>
  </tr>
  </thead>
<tbody>
  <tr>
    <th><b>LLM model</b></th>
    <td>???</td>
  </tr>
  <tr>
    <th><b>LLM model version</b></th>
    <td>???</td>
  </tr>
  <tr>
    <th><b>Model/service URL</b></th>
    <td>???</td>
  </tr>
  <tr>
    <th><b>Why this model?</b></th>
    <td>???</td>
  </tr>
</tbody>
</table>


#### Description (LLM)

_In the LLM's words, what is this paper about?_

##### Prompt

```markdown
prompt here
```

#### What are the authors proposing?

##### Prompt

```markdown
prompt here
```

#### What is the motivation for the work?

##### Prompt

```markdown
prompt here
```

#### What is the approach or innovation?

##### Prompt

```markdown
prompt here
```

#### What are the results and how do they compare with competing approaches?

##### Prompt

```markdown
prompt here
```

#### Is the comparison fair?

##### Prompt

```markdown
prompt here
```

#### What are the takeaways according to the authors?

##### Prompt

```markdown
prompt here
```

#### What are the takeaways according to you?

##### Prompt

```markdown
prompt here
```

#### Would you use this?  If so, how/where would you use this?

##### Prompt

```markdown
prompt here
```

##### Prompt

```markdown
prompt here
```

#### What problems remain and what are the next steps?

##### Prompt

```markdown
prompt here
```

### Experience using the LLM

_Describe your process for using the LLM.  How did the LLM perform?_

#### Errors and limitations of the LLM 

_Where did it fall short or make mistakes?_
